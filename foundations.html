<!DOCTYPE html>
<html lang="pt">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Foundations</title>
    <link rel="stylesheet" href="assets/css/styles.css">
</head>
<body>
    <header>
        <div class="header-container">
            <h1>Foundations</h1>
        </div>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="about_me.html">About me</a></li>
        
                <li class="dropdown">
                    <a href="#">Programming ‚ñæ</a>
                    <ul class="dropdown-content">
                        <li><a href="python.html">Python</a></li>
                        <li><a href="jinja.html">Jinja</a></li>
                        <li><a href="linux.html">Linux</a></li>
                    </ul>
                </li>

                <li class="dropdown">
                    <a href="#">Neural Networks ‚ñæ</a>
                    <ul class="dropdown-content">
                        <li><a href="foundations.html">Foundations</a></li>
                        <li><a href="classical_models.html">Classical Models</a></li>
                        <li><a href="datasets.html">Datasets</a></li>
                        <li><a href="math_tools.html">Mathematical Tools</a></li>
                    </ul>
                </li>
            </ul>
        </nav>
    </header>

    <main>
        <!-- INDEX -->
        <section>
            <h2>Index</h2>
            <ul>
                <li><a href="#optimization">1. Optimization</a></li>
                <li><a href="#grad">1.1. Gradient Descent</a></li>
                <li><a href="#sgd">1.2. Minibatch SGD</a></li>
                <li><a href="#momentum">1.3. Momentum</a></li>
                <li><a href="#adam">1.4. Adam</a></li>
                <li><a href="#rate">1.5. Learning Rate</a></li>
            </ul>
        </section>

        <section id="optimization">
            <h2>1. Optimization</h2>
            <p class="first-parag">Optimization refers to the process by which a neural network adjusts its parameters to minimize a loss function that measures prediction error. Because deep learning models operate in enormous, irregular loss landscapes, optimization relies on gradient-based algorithms that iteratively update weights in directions that reduce the loss. Modern techniques balance computational efficiency, stability, and generalization, turning the static architecture of a network into a system capable of learning meaningful patterns from data.</p>        
            <p>Optimization is the process through which a neural network learns to perform a task. Instead of manually choosing the values of the parameters, we rely on an algorithm that gradually adjusts the network‚Äôs weights so that its predictions become more accurate over time. At the heart of this process is the idea of minimizing a loss function, a mathematical expression that quantifies how far the model‚Äôs current predictions are from the desired outputs. The goal of optimization is to find the set of parameters that makes this loss as small as possible.</p>
            <p>In deep learning, the loss landscape is typically high-dimensional and complex, shaped by millions of parameters interacting through multiple layers. Because of this, exhaustive search or analytical solutions are impossible. Instead, neural networks use gradient-based methods: by computing the gradient of the loss with respect to each weight, the network can determine in which direction the parameters should move to reduce the error. This computation is performed efficiently through the backpropagation algorithm, which propagates error information backwards through the network to update each weight according to its contribution to the total loss.</p>
            <p>What makes optimization in neural networks particularly challenging is that the loss surface is full of valleys, ridges, flat regions and irregularities. This means that na√Øvely following the gradient does not guarantee fast progress or good solutions. The optimization algorithm needs to deal with issues such as slow convergence, getting trapped in poor local minima, or oscillating in regions where the gradient changes direction frequently. As a result, optimization is not just a mechanical step in training a model‚Äîit is the central mechanism that determines whether the network will actually learn.</p>
            <p>Because computing gradients over an entire dataset is often too expensive, modern training instead relies on estimating these gradients from smaller portions of the data, allowing the model to update its parameters more frequently and at a lower computational cost. This introduces stochasticity into the optimization process, making the training dynamics more irregular but also helping the model generalize better by avoiding overly rigid solutions that fit the data too precisely.</p>
            <p>Another central difficulty is choosing how large the parameter updates should be. If the steps are too small, training becomes extremely slow; if they are too large, the optimization may diverge or bounce around uncontrollably. Different optimization algorithms implement different strategies to adjust these updates, whether by smoothing the gradient information, adapting step sizes automatically, or shaping the update trajectory based on past behavior. The interaction between these strategies defines how efficiently the network learns and how well it converges.</p>
            <p>Ultimately, optimization is the engine that drives deep learning. It transforms a static architecture into a model that actually learns patterns, relationships and structures from data. Understanding the principles of optimization provides the foundation for grasping both the strengths and the limitations of neural networks, and it sets the stage for exploring more advanced training mechanisms and algorithms</p> 


        <section id="grad">
            <h3>1.1. Gradient Descent</h3>
            <p class="first-parag">Gradient Descent is the basic method for training neural networks, updating parameters by moving them in the direction that locally decreases the loss. By computing gradients through backpropagation, the optimizer determines how each weight contributes to prediction error. Although conceptually simple, Gradient Descent must navigate high-dimensional landscapes filled with local minima, flat regions, and steep slopes, making its performance highly dependent on choices like learning rate and data usage.</p>
            <p>Gradient Descent is the foundational optimization method used in training neural networks. Its purpose is to find the set of parameters that minimize a loss function, which measures how far the model‚Äôs predictions are from the desired outputs. At its core, Gradient Descent relies on a simple idea: if we can compute the slope of the loss function with respect to each parameter, then we can move the parameters in the opposite direction of this slope in order to reduce the loss. This slope is called the gradient, and it tells us how small changes in each weight affect the overall error</p>
            <p>The process begins by initializing the network‚Äôs weights, usually with small random values. After a forward pass through the network, the model produces predictions. These predictions are compared to the ground truth through the loss function, generating a numerical value that reflects the model‚Äôs performance. The next step is to compute the gradient of the loss with respect to every weight in the network. This is done efficiently using the backpropagation algorithm, which applies the chain rule of calculus to propagate errors backward through the network‚Äôs layers. Once the gradients are computed, each weight is updated by subtracting a fraction of its gradient. This fraction is controlled by a parameter called the learning rate, which determines the size of each update step.</p>
            <p>The choice of learning rate is crucial for the behavior of Gradient Descent. If the learning rate is too large, the updates may overshoot the minimum of the loss function, causing the training process to diverge or oscillate wildly. If it is too small, progress becomes extremely slow, and the optimizer may get stuck in regions where the gradient is nearly zero. This delicate balance is why learning rate tuning is often one of the most important parts of successful neural network training.</p>
            <p>Gradient Descent can be understood as navigating a high-dimensional landscape, where the height represents the loss value and the coordinates represent the parameters of the network. The optimizer attempts to move downhill, step by step, until it reaches a region of low loss. However, the landscape of deep neural networks is not a smooth bowl; it contains plateaus, narrow valleys, steep cliffs, and many local minima. Because of this complexity, the basic version of Gradient Descent, which uses the entire dataset to compute an exact gradient at each step, is rarely used in practice. It is computationally expensive and can struggle to escape certain undesirable regions of the loss surface.</p>
            <p>Instead, modern training relies on stochastic variants of Gradient Descent that approximate gradients using small subsets of the data. These methods introduce noise into the optimization process, which often helps the model escape shallow local minima and converge faster. Nevertheless, the underlying principle remains the same: compute gradients, adjust the parameters, and repeat the process until the model improves.</p>
            <p>Gradient Descent is therefore not just an algorithm‚Äîit is the fundamental mechanism that enables neural networks to learn from data. Its simplicity and mathematical elegance make it the backbone of nearly every training procedure in deep learning, and understanding it provides the foundation for grasping more advanced optimization techniques such as momentum, adaptive methods, and second-order approaches.</p>
            <li>
                More about Gradient Descent
                <a class="pdf-button" href="assets/tablet_pdfs/Gradient Descent.pdf" target="_blank">
                    <span class="pdf-icon">üìÑ</span> Gradient Descent   
                </a>
            </li>
        </section>

        <section id="sgd">
            <h3>1.2. Minibatch SGD</h3>
            <p class="first-parag">Minibatch SGD improves upon full-batch Gradient Descent by estimating gradients using small, randomly chosen subsets of data. This makes training dramatically faster and introduces helpful stochasticity that can improve generalization and escape poor local minima. The method balances efficiency and stability, forming the foundation of nearly all modern deep learning optimizers.</p>
            <p>Minibatch Stochastic Gradient Descent (often simply called ‚Äúminibatch SGD‚Äù) is the most widely used method for training neural networks in practice. It is a refinement of basic Gradient Descent that balances computational efficiency, stability, and the ability to generalize. Instead of computing the gradient of the loss function using the entire dataset at once, minibatch SGD computes the gradient using only a small subset of the data at each update step. This subset is called a minibatch, and its size is typically much smaller than the full dataset, often ranging from a dozen to a few hundred samples.</p>
            <p>The motivation for using minibatches comes from the structure of modern datasets. Large datasets can contain tens or hundreds of thousands of examples, making it computationally expensive to calculate the exact gradient for every iteration of training. Minibatch SGD circumvents this limitation by estimating the gradient from a small, randomly selected subset. This estimate is not exact, but it is statistically reliable enough to guide the learning process effectively. The randomness introduced by the minibatch selection injects controlled noise into the optimization process, which can help the network escape shallow minima or avoid becoming trapped in flat regions of the loss surface.</p>
            <p>Training with minibatches follows a simple cycle. The dataset is shuffled at the beginning of each epoch, and then divided into consecutive minibatches. For each minibatch, the network performs a forward pass to compute predictions, evaluates the loss on that subset of examples, and calculates the corresponding gradient using backpropagation. The weights are then updated based on this gradient estimate. Over time, as the optimizer moves from minibatch to minibatch, the average effect of these updates approximates the true gradient over the entire dataset, while being far more efficient to compute.</p>
            <p>The size of the minibatch has a significant impact on training dynamics. Smaller minibatches introduce more noise into the gradient estimate, which can help the model generalize better and prevent overfitting, but may cause more unstable updates. Larger minibatches result in smoother and more accurate gradient estimates, which can make training more stable but may slow down convergence or lead the model to rely too heavily on specific patterns in the training data. Choosing the right minibatch size is therefore a trade-off between computational efficiency, stability, and the quality of generalization.</p>
            <p>Minibatch SGD also serves as the foundation for more advanced optimizers such as Momentum, RMSProp, and Adam. All of these methods rely on the idea of using minibatches to estimate gradients and build additional mechanisms on top of it to improve convergence and learning stability. Despite these variations, the core principle remains the same: minibatch SGD enables neural networks to learn efficiently from large datasets by providing a practical approximation to full-batch optimization.</p>
            <p>In summary, minibatch SGD is the practical engine behind modern deep learning. It provides a balance between computational feasibility and effective learning, and its stochastic nature contributes to better generalization and more robust behavior. Understanding minibatch SGD is essential for understanding how neural networks are trained in real-world applications.</p>
        </section>

        <section id="momentum">
            <h3>1.3. Momentum</h3>
            <p class="first-parag">Momentum enhances SGD by accumulating a velocity term that reflects past gradients, allowing the optimizer to maintain direction and smooth out oscillations in the loss landscape. This creates faster convergence along shallow directions and more stable updates in regions with steep curvature, making Momentum a simple yet powerful improvement over basic gradient methods.</p>
            <p>Momentum is an extension of Stochastic Gradient Descent designed to accelerate training and to stabilize the optimization trajectory in neural networks. While basic SGD updates the parameters solely based on the current gradient, Momentum incorporates information from past updates, acting as a form of ‚Äúmemory‚Äù that helps the optimizer maintain direction in the loss landscape. This idea is inspired by classical mechanics: instead of treating each step as an isolated movement, the optimizer behaves like a particle with inertia, accumulating velocity as it descends the loss surface.</p>
            <p>The motivation behind Momentum arises from the geometry of neural network loss surfaces. Many regions contain narrow valleys: directions where the loss changes quickly and others where it changes very slowly. In such landscapes, vanilla SGD tends to oscillate from side to side along the steep dimension while making very slow progress along the shallow one. This zig-zagging wastes computation and makes convergence inefficient. Momentum mitigates this issue by smoothing the optimization path: it encourages the parameters to continue moving in directions that consistently reduce the loss, while dampening oscillations in directions where the gradient repeatedly changes sign.</p>
            <p>The mechanism of Momentum relies on maintaining a running average of past gradients. At each iteration, instead of updating the weights directly using the gradient, the optimizer updates a ‚Äúvelocity‚Äù term that combines the previous velocity with the current gradient. This velocity is then applied to the weights. The result is that updates gain persistence: if the gradients continue pointing in the same direction, the velocity grows stronger, allowing the optimizer to move faster toward minima. If the gradients fluctuate, the velocity dampens the noise, leading to more stable and predictable steps. The hyperparameter controlling this behavior is the momentum coefficient, typically a value close to 1 (such as 0.9), which determines how much influence past gradients retain over time.</p>
            <p>By accumulating velocity, Momentum helps the optimization process overcome small local minima and plateaus that would otherwise slow down simple SGD. It allows the optimizer to ‚Äúpush through‚Äù flat regions of the loss surface where gradients are weak, because the accumulated velocity maintains movement even when the current gradient is small. At the same time, it reduces unnecessary oscillations in sharp directions, improving stability and speeding up convergence. The learning rate and momentum coefficient work together to define the optimizer‚Äôs behavior: the learning rate controls the size of new gradient contributions, while the momentum coefficient dictates how strongly the optimizer follows its previous trajectory.</p>
            <p>Momentum has become a standard component of modern optimization because it provides a simple yet powerful improvement over basic SGD. It preserves the computational efficiency of minibatch training while offering smoother, faster, and more reliable convergence. Many widely used optimizers‚Äîincluding Nesterov Momentum, RMSProp, and Adam‚Äîbuild upon the core idea introduced here: using history to guide parameter updates more intelligently. Understanding Momentum is therefore essential for grasping how contemporary neural networks achieve stability and speed during training.</p>
        </section>

        <section id="adam">
            <h3>1.4. Adam</h3>
            <p class="first-parag">Adam combines the benefits of Momentum with adaptive learning rates by tracking both the mean and the variance of gradients for each parameter. With bias correction and per-parameter scaling, Adam offers fast, stable convergence even in noisy or sparse gradient regimes. Its versatility and reliability have made it one of the most widely adopted optimization algorithms in deep learning.</p>
            <p>Adam (Adaptive Moment Estimation) is one of the most widely used optimization algorithms in deep learning because it combines the benefits of two important ideas: Momentum and adaptive learning rates. Its goal is to provide fast, stable convergence even in complex, noisy, or highly non-stationary optimization landscapes. Whereas basic SGD updates each parameter using only the current gradient, Adam maintains a richer set of internal statistics that help the optimizer make better-informed decisions at every step.</p>
            <p>The foundation of Adam is the idea of tracking two types of ‚Äúmoments‚Äù of the gradient. The first moment is essentially a running average of the gradient itself, similar to the velocity term used in Momentum. This term captures the general direction in which the optimizer should move. The second moment is a running average of the squared gradient, which provides information about the gradient‚Äôs magnitude and variability. By combining these two sources of information, Adam adapts the size of each parameter update based not only on the slope of the loss surface but also on how uncertain or volatile the gradients are at each parameter.</p>
            <p>Because these running averages start at zero, Adam includes a correction mechanism known as ‚Äúbias correction,‚Äù which compensates for the fact that the early estimates of the moments tend to be underestimated. This correction ensures that the optimizer remains stable during the initial stages of training, preventing extremely small or inconsistent updates that might slow down learning. After this correction, Adam computes an adaptive learning rate for each parameter, dividing the first moment estimate by the square root of the second moment estimate. Parameters with large or unstable gradients receive smaller updates, while parameters with reliably small gradients receive larger ones. This adaptivity allows the optimizer to focus on difficult or sensitive parts of the model while avoiding instability in regions where gradients fluctuate wildly.</p>
            <p>One of Adam‚Äôs biggest strengths is its robustness in the face of noisy or sparse gradients, which appear frequently in modern deep learning tasks such as natural language processing, reinforcement learning, and large-scale vision models. Because Adam adjusts the learning rate individually for each parameter, it naturally handles situations where some parts of the network learn faster than others. This makes it far easier to train deep or irregular architectures without manually tuning learning rates by layer or by parameter group.</p>
            <p>However, Adam is not without limitations. Although it typically converges quickly, its final solutions do not always generalize as well as those obtained through SGD with Momentum in large-scale supervised learning tasks. This trade-off has led to adaptations such as AdamW, which modifies Adam‚Äôs weight-decay behavior to improve generalization by decoupling regularization from the adaptive update rule. Even so, the original Adam remains a cornerstone of modern optimization due to its reliability, speed, and ease of use.</p>
            <p>In essence, Adam stands at the intersection of smoothness and adaptability. It stabilizes the optimization path through moment accumulation while flexibly adjusting learning rates to local curvature and gradient variability. This combination makes Adam exceptionally effective for a wide range of tasks, and understanding its mechanics helps clarify how modern deep learning models can be trained efficiently even in the presence of noisy data, complex architectures, and high-dimensional parameter spaces.</p>
        </section>

        <section id="rate">
            <h3>1.5. Learning Rate and Learning Rate Scheduling</h3>
            <p class="first-parag">The learning rate controls the size of parameter updates during training, shaping how quickly or cautiously a model learns. Because different stages of training benefit from different step sizes, learning rate schedules adjust this value over time‚Äîvia decay, warmup, or adaptive triggers‚Äîto balance exploration and fine-tuning. Effective scheduling is essential for stable and efficient neural network training.</p>
            <p>The learning rate is one of the most influential hyperparameters in neural network optimization. It determines the size of each update step during training, controlling how quickly or cautiously the model adjusts its parameters in response to the computed gradients. A high learning rate makes the optimizer take bold steps through the loss landscape, potentially speeding up convergence but also risking overshooting minima or diverging entirely. A low learning rate leads to more careful updates, which can improve stability but may slow progress to a crawl, especially in flat regions of the loss surface where gradients carry little information. Because the geometry of deep learning loss surfaces is highly irregular‚Äîfull of steep walls, shallow basins, plateaus, and winding valleys‚Äîthe choice of learning rate directly shapes the entire training trajectory.</p>
            <p>A fixed learning rate throughout training is rarely ideal. Early in training, the optimizer benefits from large steps that allow it to explore the parameter space aggressively, escaping poor initial regions and quickly reducing gross errors. As training progresses, however, the model enters more refined regions of the loss landscape where gradient information becomes subtler. At this stage, large updates can destabilize convergence or prevent the optimizer from settling into a good local minimum. For these reasons, modern neural network training strategies rely on learning rate schedules‚Äîpredefined or adaptive mechanisms that adjust the learning rate over time to better match the evolving needs of the optimization process.</p>
            <p>Learning rate scheduling can take many forms. One of the most common approaches is step decay, where the learning rate is reduced by a fixed factor at specific epochs. This creates a training dynamic that begins with rapid progress and gradually shifts toward fine-tuning. Another popular strategy is exponential decay, where the learning rate decreases smoothly over time, providing a more continuous transition between exploration and refinement. More sophisticated schedules, such as cosine annealing, introduce periodic cycles in which the learning rate decreases and then briefly increases again. These cycles help the optimizer escape narrow or suboptimal minima by periodically injecting more exploratory behavior.</p>
            <p>Warmup phases are another important component of many modern schedules. In a warmup, the learning rate begins at a very small value and gradually increases during the first few iterations or epochs. This prevents unstable updates at the very beginning of training, when gradients are often large and the model has no prior information to stabilize its behavior. Warmup has become especially important in large-scale models‚Äîsuch as Transformers‚Äîwhere initial instability can severely hinder convergence.</p>
            <p>Adaptive scheduling methods take a different approach. Instead of adjusting the learning rate according to a predetermined timeline, they react to the observed behavior of the optimization process. For example, the ‚ÄúReduce on Plateau‚Äù strategy lowers the learning rate when the validation loss stops improving, allowing the optimizer to adjust its pace automatically based on performance. Combined with adaptive optimizers like Adam or RMSProp, these scheduling strategies create a highly dynamic learning process that adapts to the curvature and difficulty of different regions of the loss surface.</p>
            <p>Ultimately, learning rate scheduling is a way of orchestrating the rhythm of learning‚Äîfast enough to make rapid progress, slow enough to converge reliably. Without a well-chosen schedule, even powerful models and sophisticated optimizers can underperform or fail altogether. Understanding how learning rates evolve during training provides deep insight into why certain optimization strategies succeed and how to tailor them to specific tasks, architectures, and datasets.</p>
        </section>

    
    
    </section>
    </main>
 

    <footer>
        <p>&copy; 2025 Ana Princival</p>
    </footer>
</body>
</html>
