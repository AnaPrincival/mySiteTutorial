<!DOCTYPE html>
<html lang="pt">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Foundations</title>
    <link rel="stylesheet" href="assets/css/styles.css">
</head>
<body>
    <header>
        <div class="header-container">
            <h1>Foundations</h1>
        </div>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="about_me.html">About me</a></li>
        
                <li class="dropdown">
                    <a href="#">Programming ▾</a>
                    <ul class="dropdown-content">
                        <li><a href="python.html">Python</a></li>
                        <li><a href="jinja.html">Jinja</a></li>
                        <li><a href="linux.html">Linux</a></li>
                    </ul>
                </li>

                <li class="dropdown">
                    <a href="#">Neural Networks ▾</a>
                    <ul class="dropdown-content">
                        <li><a href="foundations.html">Foundations</a></li>
                        <li><a href="BNN.html">Bayesian</a></li>
                        <li><a href="classical_models.html">Classical Models</a></li>
                        <li><a href="datasets.html">Datasets</a></li>
                        <li><a href="math_tools.html">Mathematical Tools</a></li>
                    </ul>
                </li>
            </ul>
        </nav>
    </header>

    <main>
        <section>
            <h2>Optimization</h2>
            <p>Optimization refers to the process by which a neural network adjusts its parameters to minimize a loss function that measures prediction error. Because deep learning models operate in enormous, irregular loss landscapes, optimization relies on gradient-based algorithms that iteratively update weights in directions that reduce the loss. Modern techniques balance computational efficiency, stability, and generalization, turning the static architecture of a network into a system capable of learning meaningful patterns from data.</p>        </section>
            <p>Optimization is the process through which a neural network learns to perform a task. Instead of manually choosing the values of the parameters, we rely on an algorithm that gradually adjusts the network’s weights so that its predictions become more accurate over time. At the heart of this process is the idea of minimizing a loss function, a mathematical expression that quantifies how far the model’s current predictions are from the desired outputs. The goal of optimization is to find the set of parameters that makes this loss as small as possible.</p>
            <p>In deep learning, the loss landscape is typically high-dimensional and complex, shaped by millions of parameters interacting through multiple layers. Because of this, exhaustive search or analytical solutions are impossible. Instead, neural networks use gradient-based methods: by computing the gradient of the loss with respect to each weight, the network can determine in which direction the parameters should move to reduce the error. This computation is performed efficiently through the backpropagation algorithm, which propagates error information backwards through the network to update each weight according to its contribution to the total loss.</p>
            <p>What makes optimization in neural networks particularly challenging is that the loss surface is full of valleys, ridges, flat regions and irregularities. This means that naïvely following the gradient does not guarantee fast progress or good solutions. The optimization algorithm needs to deal with issues such as slow convergence, getting trapped in poor local minima, or oscillating in regions where the gradient changes direction frequently. As a result, optimization is not just a mechanical step in training a model—it is the central mechanism that determines whether the network will actually learn.</p>
            <p>Because computing gradients over an entire dataset is often too expensive, modern training instead relies on estimating these gradients from smaller portions of the data, allowing the model to update its parameters more frequently and at a lower computational cost. This introduces stochasticity into the optimization process, making the training dynamics more irregular but also helping the model generalize better by avoiding overly rigid solutions that fit the data too precisely.</p>
            <p>Another central difficulty is choosing how large the parameter updates should be. If the steps are too small, training becomes extremely slow; if they are too large, the optimization may diverge or bounce around uncontrollably. Different optimization algorithms implement different strategies to adjust these updates, whether by smoothing the gradient information, adapting step sizes automatically, or shaping the update trajectory based on past behavior. The interaction between these strategies defines how efficiently the network learns and how well it converges.</p>
            <p>Ultimately, optimization is the engine that drives deep learning. It transforms a static architecture into a model that actually learns patterns, relationships and structures from data. Understanding the principles of optimization provides the foundation for grasping both the strengths and the limitations of neural networks, and it sets the stage for exploring more advanced training mechanisms and algorithms</p> 
        </section>

        <section>
            <h2>Gradient Descent</h2>
            <p>Gradient Descent is the basic method for training neural networks, updating parameters by moving them in the direction that locally decreases the loss. By computing gradients through backpropagation, the optimizer determines how each weight contributes to prediction error. Although conceptually simple, Gradient Descent must navigate high-dimensional landscapes filled with local minima, flat regions, and steep slopes, making its performance highly dependent on choices like learning rate and data usage.</p>
            <p>Gradient Descent is the foundational optimization method used in training neural networks. Its purpose is to find the set of parameters that minimize a loss function, which measures how far the model’s predictions are from the desired outputs. At its core, Gradient Descent relies on a simple idea: if we can compute the slope of the loss function with respect to each parameter, then we can move the parameters in the opposite direction of this slope in order to reduce the loss. This slope is called the gradient, and it tells us how small changes in each weight affect the overall error</p>
            <p>The process begins by initializing the network’s weights, usually with small random values. After a forward pass through the network, the model produces predictions. These predictions are compared to the ground truth through the loss function, generating a numerical value that reflects the model’s performance. The next step is to compute the gradient of the loss with respect to every weight in the network. This is done efficiently using the backpropagation algorithm, which applies the chain rule of calculus to propagate errors backward through the network’s layers. Once the gradients are computed, each weight is updated by subtracting a fraction of its gradient. This fraction is controlled by a parameter called the learning rate, which determines the size of each update step.</p>
            <p>The choice of learning rate is crucial for the behavior of Gradient Descent. If the learning rate is too large, the updates may overshoot the minimum of the loss function, causing the training process to diverge or oscillate wildly. If it is too small, progress becomes extremely slow, and the optimizer may get stuck in regions where the gradient is nearly zero. This delicate balance is why learning rate tuning is often one of the most important parts of successful neural network training.</p>
            <p>Gradient Descent can be understood as navigating a high-dimensional landscape, where the height represents the loss value and the coordinates represent the parameters of the network. The optimizer attempts to move downhill, step by step, until it reaches a region of low loss. However, the landscape of deep neural networks is not a smooth bowl; it contains plateaus, narrow valleys, steep cliffs, and many local minima. Because of this complexity, the basic version of Gradient Descent, which uses the entire dataset to compute an exact gradient at each step, is rarely used in practice. It is computationally expensive and can struggle to escape certain undesirable regions of the loss surface.</p>
            <p>Instead, modern training relies on stochastic variants of Gradient Descent that approximate gradients using small subsets of the data. These methods introduce noise into the optimization process, which often helps the model escape shallow local minima and converge faster. Nevertheless, the underlying principle remains the same: compute gradients, adjust the parameters, and repeat the process until the model improves.</p>
            <p>Gradient Descent is therefore not just an algorithm—it is the fundamental mechanism that enables neural networks to learn from data. Its simplicity and mathematical elegance make it the backbone of nearly every training procedure in deep learning, and understanding it provides the foundation for grasping more advanced optimization techniques such as momentum, adaptive methods, and second-order approaches.</p>
        </section>
        
    
    
    
    </main>
 

    <footer>
        <p>&copy; 2025 Ana Princival</p>
    </footer>
</body>
</html>
