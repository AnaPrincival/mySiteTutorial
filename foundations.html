<!DOCTYPE html>
<html lang="pt">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Foundations</title>
    <link rel="stylesheet" href="assets/css/styles.css">
</head>
<body>
    <header>
        <div class="header-container">
            <h1>Foundations</h1>
        </div>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="about_me.html">About me</a></li>
        
                <li class="dropdown">
                    <a href="#">Programming ‚ñæ</a>
                    <ul class="dropdown-content">
                        <li><a href="python.html">Python</a></li>
                        <li><a href="jinja.html">Jinja</a></li>
                        <li><a href="linux.html">Linux</a></li>
                    </ul>
                </li>

                <li class="dropdown">
                    <a href="#">Neural Networks ‚ñæ</a>
                    <ul class="dropdown-content">
                        <li><a href="foundations.html">Foundations</a></li>
                        <li><a href="classical_models.html">Classical Models</a></li>
                        <li><a href="datasets.html">Datasets</a></li>
                        <li><a href="math_tools.html">Mathematical Tools</a></li>
                    </ul>
                </li>
            </ul>
        </nav>
    </header>

    <main>
        <!-- INDEX -->
        <section>
            <h2>Index</h2>
            <ul>
                <li><a href="#optimization">1. Optimization</a></li>
                <li><a href="#grad">1.1. Gradient Descent</a></li>
                <li><a href="#sgd">1.2. Minibatch SGD</a></li>
                <li><a href="#momentum">1.3. Momentum</a></li>
                <li><a href="#adam">1.4. Adam</a></li>
                <li><a href="#rate">1.5. Learning Rate</a></li>
                <li><a href="#regularization">2. Regularization</a></li>
                <li><a href="#over">2.1. Overfitting</a></li>
                <li><a href="#under">2.2. Underfitting</a></li>
                <li><a href="#dropout">2.3. Dropout</a></li>
                <li><a href="#lasso">2.4. L1 (LASSO)</a></li>
                <li><a href="#ridge">2.5. L2 (Weight Decay or Ridge)</a></li>

            </ul>
        </section>

        <section id="optimization">
            <h2>1. Optimization</h2>
            <p class="first-parag">Optimization is the mechanism that allows a neural network to learn. It adjusts the model‚Äôs parameters so that predictions become closer to the desired outputs, guided by a loss function that measures how far the model is from being correct. In other words, optimization is the process that transforms a fixed architecture into a system that adapts to data.</p>        
            <p>Neural networks rely on gradient-based methods because the loss landscape is extremely high-dimensional and complex. Backpropagation computes how the loss changes with respect to each weight, allowing the model to update its parameters in directions that reduce error. Since evaluating the entire dataset at once would be too expensive, modern training uses minibatches, introducing some randomness into the process but greatly improving efficiency.</p>
            <p>The challenge is that the loss surface contains flat regions, steep valleys, oscillations, and many irregularities. Simply following the gradient may lead to slow progress or unstable behavior. For this reason, optimization algorithms incorporate strategies to smooth updates, adapt learning rates, or use information from past iterations to guide the trajectory more effectively.</p>
            <p>Another difficulty is choosing how large each update should be. Small steps make training slow, while large steps can cause divergence. Different optimization methods balance this trade-off in different ways, shaping how quickly and reliably the model converges.</p>
            <p>Ultimately, optimization determines whether a neural network actually learns useful patterns. It is the foundation upon which all other training techniques‚Äîsuch as momentum, adaptive optimizers, and scheduling methods‚Äîare built.</p>


        <section id="grad">
            <h3>1.1. Gradient Descent</h3>
            <p class="first-parag">Gradient Descent is the basic algorithm that guides how a neural network updates its parameters. It adjusts the weights in the direction that most reduces the loss, using the gradient as a compass. The goal is simple: move step by step toward values that make the model‚Äôs predictions more accurate.</p>
            <img src="assets/img/gradient.png" alt="gradient" class="small-section-image"> 
            <p>At the core of Gradient Descent is the gradient, which measures how the loss changes with respect to each parameter. If increasing a weight makes the loss grow, the gradient will indicate that the weight should decrease, and vice versa. Backpropagation allows the network to compute all these gradients efficiently, even for millions of parameters.</p>
            <p>The algorithm updates each weight by taking a step proportional to the gradient and to a scalar known as the learning rate. This learning rate determines how far the algorithm moves at each update: small steps lead to slow but stable progress, while large steps may cause oscillations or divergence. Choosing an appropriate learning rate is therefore essential for training to succeed.</p>
            <p>Although Gradient Descent is conceptually simple, the loss surface of deep networks is irregular and full of twists, which makes the descent pathway far from smooth. Flat regions can stall progress, steep directions can make updates unstable, and noisy gradients (from minibatches) can push the model in imperfect directions. Still, despite these limitations, Gradient Descent remains the foundation for virtually all modern optimization methods in deep learning.</p>
            <li>
                More about Gradient Descent
                <a class="pdf-button" href="assets/tablet_pdfs/Gradient Descent.pdf" target="_blank">
                    <span class="pdf-icon">üìÑ</span> Gradient Descent   
                </a>
            </li>
        </section>

        <section id="sgd">
            <h3>1.2. Minibatch SGD</h3>
            <p class="first-parag">Minibatch SGD is a practical version of Gradient Descent that estimates the gradient using only a small portion of the dataset at each update. It makes training faster, cheaper, and more responsive, while still guiding the model in the right direction toward minimizing the loss.</p>
            <p>Instead of computing the gradient over the full dataset‚Äîwhich is often too large to process at once‚Äîthe algorithm selects a small batch of examples and uses them to approximate the true gradient. Although this introduces randomness into the training path, the estimate is usually accurate enough to steer learning effectively. The stochasticity also helps the model escape overly sharp or overly specific solutions, improving generalization.</p>
            <p>Because updates occur after every minibatch rather than after the entire dataset, the model learns continuously. This leads to faster feedback between prediction and correction, letting the network adjust its parameters more frequently. The computational cost is also reduced: minibatches fit into memory more easily and are well suited for parallel hardware like GPUs.</p>
            <p>The noise introduced by minibatch sampling, however, makes the optimization path more irregular. Instead of a smooth descent, the loss curve oscillates as the algorithm reacts to slightly different gradients at each step. Still, this variability is often beneficial, preventing the model from getting stuck in unhelpful regions of the loss landscape and making learning more robust.</p>
            <p>Minibatch SGD therefore becomes the backbone of modern deep learning training. It balances efficiency, stability, and generalization, providing a practical compromise between full Gradient Descent and purely stochastic updates.</p>
        </section>

        <section id="momentum">
            <h3>1.3. Momentum</h3>
            <p class="first-parag">Momentum is an extension of Gradient Descent that makes learning smoother and faster by accumulating information from past updates. Instead of reacting only to the current gradient, the algorithm builds a sense of direction over time, helping the model move consistently toward lower loss.</p>
            <img src="assets/img/momentum.jpg" alt="momentum" class="small-section-image"> 
            <p>The core idea is simple: each update is influenced not only by the present gradient but also by a fraction of the previous update. This accumulated ‚Äúvelocity‚Äù allows the optimization to flow through shallow regions and resist oscillations in directions where the gradient frequently changes sign. As a result, Momentum tends to accelerate progress along stable paths and dampen noisy, zigzag movements.</p>
            <p>Mathematically, the algorithm maintains a velocity term that keeps track of past gradients. When the model encounters a long, gentle slope in the loss landscape, this velocity grows and pushes the network forward more effectively than plain Gradient Descent. Conversely, in directions where the gradient fluctuates or forms steep walls, the accumulated momentum slows down the updates, providing stability and preventing erratic jumps.</p>
            <p>Momentum is particularly helpful when the loss surface has narrow valleys‚Äîcommon in deep networks‚Äîwhere one direction is steep and the other is shallow. Plain Gradient Descent wastes time oscillating across the steep direction, advancing very slowly along the shallow one. By remembering the consistent component of the gradient, Momentum helps the model move forward steadily instead of vibrating sideways.</p>
            <p>Overall, Momentum improves both the speed and the reliability of training. It gives the optimization process a sense of inertia, allowing the network to advance through difficult regions of the loss landscape with more confidence and less noise.</p>
        </section>

        <section id="adam">
            <h3>1.4. Adam</h3>
            <p class="first-parag">Adam (Adaptive Moment Estimation) is an adaptive optimization algorithm that adjusts both the direction and the size of each weight update. It combines the stability of Momentum with the flexibility of per-parameter learning rates, allowing neural networks to train efficiently even in complex, noisy loss landscapes.</p>
            <p>Adam keeps two moving averages during training: one for the gradients themselves (capturing direction, like Momentum) and another for the squared gradients (capturing how large or variable they are). These two estimates help the optimizer decide not only where to move, but how big each step should be. Parameters with consistently large gradients receive smaller updates, while parameters with small or unreliable gradients are allowed to move more freely.</p>
            <p>Because Adam adapts the learning rate for each individual weight, it handles situations where different parameters evolve at very different scales. This is especially useful in deep networks, where some layers may require tiny adjustments while others benefit from more aggressive steps. The combination of momentum-like smoothing and adaptive scaling helps the optimizer remain stable even in highly irregular regions of the loss surface.</p>
            <p>Another advantage is that Adam works well with noisy gradient estimates, such as those produced by minibatches. The algorithm filters this noise through its moving averages, producing updates that are more consistent and less sensitive to momentary fluctuations. This makes training faster to converge and often more robust.</p>
            <p>In practice, Adam has become one of the most widely used optimizers in deep learning because it requires little tuning and performs reliably across many architectures and tasks. Although not always the best choice for final performance, its convenience and stability make it a common default in modern workflows.</p>
        </section>

        <section id="rate">
            <h3>1.5. Learning Rate and Learning Rate Scheduling</h3>
            <p class="first-parag">Learning Rate Scheduling is the strategy of adjusting the learning rate during training instead of keeping it fixed. The goal is simple: start with steps large enough to make fast progress, and gradually reduce them to allow the model to refine its solution without overshooting or oscillating.</p>
            <p>During the early stages of training, a relatively high learning rate helps the optimizer explore the loss landscape quickly, covering large distances and escaping shallow traps. However, as the model approaches regions of lower loss, these large steps become counterproductive. They can cause instability, prevent convergence, or bounce the parameters around a good solution without ever settling. Scheduling solves this by steadily decreasing the learning rate as training progresses, enabling both speed and precision.</p>
            <p>There are many scheduling strategies. Some reduce the learning rate at fixed intervals, while others respond directly to training performance‚Äîfor instance, lowering the rate when the loss plateaus. More advanced approaches modify the rate in smooth curves, such as exponential decay or cosine annealing, allowing the training dynamics to evolve more gradually. Despite their differences, all schedules share the same intention: match the step size to the needs of each phase of learning.</p>
            <p>A well-chosen schedule can dramatically improve training stability and final accuracy. It helps avoid the pitfalls of a single fixed learning rate, which is often too large for late training and too small for early exploration. By controlling how aggressively the optimizer moves through the loss landscape, learning rate scheduling becomes a key mechanism for efficient and reliable convergence.</p>
        </section>
    </section>

    <section id="regularization">
        <h2>2. Regularization</h2>
        <p class="first-parag">Regularization refers to the set of techniques used to prevent a neural network from memorizing the training data instead of learning meaningful patterns. Its goal is to guide the model toward solutions that generalize well, reducing overfitting by discouraging overly complex or unstable parameter configurations.</p>        
        <p>In deep learning, a network with many parameters can easily fit noise, outliers, or accidental correlations present in the training set. When this happens, the model performs extremely well on the data it has seen but fails to make accurate predictions on new data. Regularization works by introducing constraints or penalties that push the model toward simpler, more robust representations. Instead of freely adjusting all parameters to match every detail in the training set, the network is encouraged to find smoother, more stable connections between inputs and outputs.</p>
        <p>These constraints can be applied in different ways: by modifying the loss function, altering how parameters evolve during training, or introducing controlled randomness in the learning process. Although each regularization method uses a different mechanism, they all share the same purpose: prevent the network from relying on features that are too specific to the training data.</p>
        <p>Regularization is fundamental because high-capacity models‚Äîespecially deep networks‚Äîare naturally prone to overfitting. Without some form of regularization, even large datasets may not be enough to guide the model toward the underlying structure of the problem. By shaping the training dynamics and limiting unnecessary complexity, regularization enables neural networks to achieve better accuracy, stability, and reliability when applied to real-world data.</p>

        <section id="over">
            <h3>2.1. Overfitting</h3>
            <p class="first-parag">Overfitting occurs when a neural network learns the training data too precisely, capturing noise, outliers, and irrelevant patterns instead of the underlying structure of the problem. A model that overfits performs extremely well on the data it has already seen but fails to generalize to new examples.</p>
            <img src="assets/img/over.jpg" alt="overfitting" class="small-section-image"> 
            <p>This happens because deep networks have enough capacity to memorize almost anything presented to them. When the training set contains accidental correlations, measurement errors, or rare cases that do not reflect the true distribution of the data, an unconstrained model can adapt its parameters to reproduce these details perfectly. As a result, the loss on the training set continues to decrease, while performance on validation or test data begins to degrade‚Äîa classic sign that the model is not learning meaningful patterns.</p>
            <p>Overfitting also tends to produce unstable and overly complex decision boundaries. Instead of forming smooth relationships between inputs and outputs, the model twists itself around the specific samples it has encountered, creating fragile representations that easily break when exposed to new conditions. This fragility is usually invisible during training but becomes evident when the model faces slightly different data.</p>
            <p>To avoid this scenario, regularization techniques are essential. They limit unnecessary complexity, introduce constraints, or add controlled randomness that prevents the model from relying on overly specific features. Techniques such as dropout, L1 and L2 penalties, early stopping, and data augmentation act in different ways but share the same purpose: help the network focus on patterns that generalize.</p>
            <p>Overfitting is therefore not a failure of the model, but a natural consequence of its expressive power. Understanding how and why it occurs is a key step in training neural networks that are not just accurate on paper, but reliable in practice.</p>
        </section>
        
        <section id="under">
            <h3>2.2. Underfitting</h3>
            <p class="first-parag">Overfitting occurs when a neural network learns the training data too precisely, capturing noise, outliers, and irrelevant patterns instead of the underlying structure of the problem. A model that overfits performs extremely well on the data it has already seen but fails to generalize to new examples.</p>
            <img src="assets/img/under.jpg" alt="underfitting" class="small-section-image"> 
            <p>This situation often arises when the model is too simple relative to the complexity of the task. A network with too few layers or too few neurons may lack the representational power required to approximate the target function. In such cases, even extensive training cannot overcome the architectural limitations, and the model remains stuck producing generic or overly smoothed predictions.</p>
            <p>Underfitting can also occur when training is insufficient or poorly configured. An overly small number of epochs may prevent the network from refining its parameters, while a learning rate that is too high may cause the optimization process to skip over meaningful solutions. Excessive regularization can also push the model to oversimplify its internal representations, effectively constraining it more than necessary.</p>
            <p>The symptoms of underfitting are straightforward: high training loss, high validation loss, and little improvement over time. Unlike overfitting, where performance diverges between training and validation sets, underfitting shows consistently weak results across both. Addressing it typically involves increasing model capacity, extending training, tuning hyperparameters, or reducing unnecessary regularization.</p>
            <p>Underfitting highlights the balance required in designing neural networks. A model must be complex enough to capture true patterns in the data, yet still capable of generalization. When a network underfits, the solution lies in giving it the expressive power and training conditions needed to learn effectively.</p>        
        </section>

        <section id="dropout">
            <h3>2.3. Dropout</h3>
            <p class="first-parag">Dropout is a regularization technique that prevents overfitting by temporarily removing a random subset of neurons during training. By forcing the network to learn in the presence of missing units, it encourages more robust and distributed representations instead of relying on specific pathways or co-adapted features.</p>
            <p>During each training step, dropout selects a fraction of neurons and sets their activations to zero. These neurons do not contribute to forward propagation, nor do they receive gradient updates. Because a different subset is removed at every iteration, the network effectively trains many slightly different architectures in parallel. This randomness weakens the model‚Äôs tendency to depend too heavily on any particular neuron or weight, improving generalization on unseen data.</p>
            <p>One of the key benefits of dropout is that it promotes redundancy: multiple neurons learn to capture similar information so that the model can perform well even when some units are absent. This makes the learned features more stable and reduces the likelihood that the network memorizes noise in the training set. When dropout is disabled at test time, all neurons are active again, and their combined output tends to be more reliable than that of any single sub-model.</p>
            <p>Although dropout introduces noise into the training process, this noise is precisely what helps the model generalize. It acts as a form of implicit ensemble learning, where many thinned versions of the network contribute to the final prediction. As a result, dropout is particularly effective in large architectures prone to overfitting, such as fully connected layers in deep neural networks.</p>
        </section>

        <section id = "lasso">
            <h3>2.4. L1 (LASSO)</h3>
            <p class="first-parag">L1 regularization, also known as LASSO (Least Absolute Shrinkage and Selection Operator), penalizes the absolute value of each weight in a neural network. Its defining effect is that it pushes many weights exactly to zero, encouraging the model to use only the most relevant features and producing sparse representations.</p>
            <p>In practice, L1 regularization works by adding a term proportional to the sum of the absolute weights to the loss function. This penalty makes it costly for the model to maintain large or unnecessary parameters, so during training the optimizer naturally prefers solutions where many weights shrink toward zero. Because the absolute-value penalty is not smooth at zero, it creates a ‚Äúpull‚Äù that can eliminate entire weights instead of merely reducing them.</p>
            <p>This sparsity has two benefits. First, it reduces the tendency of the model to overfit by preventing it from distributing small, noisy adjustments across many parameters. Second, it effectively performs feature selection: the model learns to rely only on the most informative inputs or internal connections, simplifying its structure and improving interpretability.</p>
            <p>L1 regularization is especially useful when the dataset contains many features but only a subset is truly important. By driving irrelevant weights to zero, LASSO helps reveal the underlying structure of the data while keeping the model compact. However, because the penalty treats all weights equally, it can sometimes be more unstable or sensitive to noise compared to L2 regularization.</p>
            <p>Despite these nuances, L1 remains a powerful tool when sparsity and feature selection are desirable. It complements other regularization techniques by directly shaping the architecture into a leaner, more focused version of itself.</p>
        </section>

        <section id = "ridge">
            <h3>2.5. L2 (Weight Decay or Ridge)</h3>
            <p class="first-parag">L2 regularization, often called Weight Decay or Ridge, penalizes the squared magnitude of the weights in a neural network. Instead of driving parameters to zero like L1, L2 gently shrinks them, discouraging large or unstable values while keeping the overall structure of the model intact.</p>
            <p>In practice, L1 regularization works by adding a term proportional to the sum of the absolute weights to the loss function. This penalty makes it costly for the model to maintain large or unnecessary parameters, so during training the optimizer naturally prefers solutions where many weights shrink toward zero. Because the absolute-value penalty is not smooth at zero, it creates a ‚Äúpull‚Äù that can eliminate entire weights instead of merely reducing them.</p>
            <p>This smooth decay stabilizes training by preventing the model from relying too heavily on specific connections. Large weights can make a network overly sensitive to small changes in the input, amplifying noise and leading to poor generalization. By keeping weights smaller and more evenly distributed, L2 regularization promotes representations that are more robust and less dependent on individual features.</p>
            <p>Weight Decay is especially useful in deep networks because it acts as a gentle form of complexity control. Instead of removing features outright, it nudges the model toward more balanced and moderate parameter values, which often improves validation accuracy and reduces overfitting. Many optimizers, such as SGD and Adam (via decoupled variants), include built-in support for this technique.</p>
            <p>Overall, L2 regularization provides a smooth, reliable way to limit model complexity without altering the architecture. It keeps the learning dynamics stable and helps neural networks generalize better to unseen data.</p>
        </section>

        <p>L1 regularization forms a diamond-shaped constraint region with corners aligned to the axes, which makes it likely that the optimal solution lies on one of these axes‚Äîforcing some coefficients to zero. L2 creates a circular region, whose smooth boundary shrinks weights but rarely drives them exactly to zero.</p>
        <img src="assets/img/l1_l2.jpg" alt="regularization" class="small-section-image"> 

    </section>


    </main>
 

    <footer>
        <p>&copy; 2025 Ana Princival</p>
    </footer>
</body>
</html>
