<!DOCTYPE html>
<html lang="pt">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Foundations</title>
    <link rel="stylesheet" href="assets/css/styles.css">
</head>
<body>
    <header>
        <div class="header-container">
            <h1>Foundations</h1>
        </div>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="about_me.html">About me</a></li>
        
                <li class="dropdown">
                    <a href="#">Programming ‚ñæ</a>
                    <ul class="dropdown-content">
                        <li><a href="python.html">Python</a></li>
                        <li><a href="jinja.html">Jinja</a></li>
                        <li><a href="linux.html">Linux</a></li>
                    </ul>
                </li>

                <li class="dropdown">
                    <a href="#">Neural Networks ‚ñæ</a>
                    <ul class="dropdown-content">
                        <li><a href="foundations.html">Foundations</a></li>
                        <li><a href="classical_models.html">Classical Models</a></li>
                        <li><a href="datasets.html">Datasets</a></li>
                        <li><a href="math_tools.html">Mathematical Tools</a></li>
                    </ul>
                </li>
            </ul>
        </nav>
    </header>

    <main>
        <!-- INDEX -->
        <section>
            <h2>Index</h2>
            <ul>
                <li><a href="#optimization">1. Optimization</a></li>
                <li><a href="#grad">1.1. Gradient Descent</a></li>
                <li><a href="#sgd">1.2. Minibatch SGD</a></li>
                <li><a href="#momentum">1.3. Momentum</a></li>
                <li><a href="#adam">1.4. Adam</a></li>
                <li><a href="#rate">1.5. Learning Rate</a></li>
            </ul>
        </section>

        <section id="optimization">
            <h2>1. Optimization</h2>
            <p class="first-parag">Optimization is the mechanism that allows a neural network to learn. It adjusts the model‚Äôs parameters so that predictions become closer to the desired outputs, guided by a loss function that measures how far the model is from being correct. In other words, optimization is the process that transforms a fixed architecture into a system that adapts to data.</p>        
            <p>Neural networks rely on gradient-based methods because the loss landscape is extremely high-dimensional and complex. Backpropagation computes how the loss changes with respect to each weight, allowing the model to update its parameters in directions that reduce error. Since evaluating the entire dataset at once would be too expensive, modern training uses minibatches, introducing some randomness into the process but greatly improving efficiency.</p>
            <p>The challenge is that the loss surface contains flat regions, steep valleys, oscillations, and many irregularities. Simply following the gradient may lead to slow progress or unstable behavior. For this reason, optimization algorithms incorporate strategies to smooth updates, adapt learning rates, or use information from past iterations to guide the trajectory more effectively.</p>
            <p>Another difficulty is choosing how large each update should be. Small steps make training slow, while large steps can cause divergence. Different optimization methods balance this trade-off in different ways, shaping how quickly and reliably the model converges.</p>
            <p>Ultimately, optimization determines whether a neural network actually learns useful patterns. It is the foundation upon which all other training techniques‚Äîsuch as momentum, adaptive optimizers, and scheduling methods‚Äîare built.</p>


        <section id="grad">
            <h3>1.1. Gradient Descent</h3>
            <p class="first-parag">Gradient Descent is the basic algorithm that guides how a neural network updates its parameters. It adjusts the weights in the direction that most reduces the loss, using the gradient as a compass. The goal is simple: move step by step toward values that make the model‚Äôs predictions more accurate.</p>
            <img src="assets/img/gradient.png" alt="gradient" class="small-section-image"> 
            <p>At the core of Gradient Descent is the gradient, which measures how the loss changes with respect to each parameter. If increasing a weight makes the loss grow, the gradient will indicate that the weight should decrease, and vice versa. Backpropagation allows the network to compute all these gradients efficiently, even for millions of parameters.</p>
            <p>The algorithm updates each weight by taking a step proportional to the gradient and to a scalar known as the learning rate. This learning rate determines how far the algorithm moves at each update: small steps lead to slow but stable progress, while large steps may cause oscillations or divergence. Choosing an appropriate learning rate is therefore essential for training to succeed.</p>
            <p>Although Gradient Descent is conceptually simple, the loss surface of deep networks is irregular and full of twists, which makes the descent pathway far from smooth. Flat regions can stall progress, steep directions can make updates unstable, and noisy gradients (from minibatches) can push the model in imperfect directions. Still, despite these limitations, Gradient Descent remains the foundation for virtually all modern optimization methods in deep learning.</p>
            <li>
                More about Gradient Descent
                <a class="pdf-button" href="assets/tablet_pdfs/Gradient Descent.pdf" target="_blank">
                    <span class="pdf-icon">üìÑ</span> Gradient Descent   
                </a>
            </li>
        </section>

        <section id="sgd">
            <h3>1.2. Minibatch SGD</h3>
            <p class="first-parag">Minibatch SGD is a practical version of Gradient Descent that estimates the gradient using only a small portion of the dataset at each update. It makes training faster, cheaper, and more responsive, while still guiding the model in the right direction toward minimizing the loss.</p>
            <p>Instead of computing the gradient over the full dataset‚Äîwhich is often too large to process at once‚Äîthe algorithm selects a small batch of examples and uses them to approximate the true gradient. Although this introduces randomness into the training path, the estimate is usually accurate enough to steer learning effectively. The stochasticity also helps the model escape overly sharp or overly specific solutions, improving generalization.</p>
            <p>Because updates occur after every minibatch rather than after the entire dataset, the model learns continuously. This leads to faster feedback between prediction and correction, letting the network adjust its parameters more frequently. The computational cost is also reduced: minibatches fit into memory more easily and are well suited for parallel hardware like GPUs.</p>
            <p>The noise introduced by minibatch sampling, however, makes the optimization path more irregular. Instead of a smooth descent, the loss curve oscillates as the algorithm reacts to slightly different gradients at each step. Still, this variability is often beneficial, preventing the model from getting stuck in unhelpful regions of the loss landscape and making learning more robust.</p>
            <p>Minibatch SGD therefore becomes the backbone of modern deep learning training. It balances efficiency, stability, and generalization, providing a practical compromise between full Gradient Descent and purely stochastic updates.</p>
        </section>

        <section id="momentum">
            <h3>1.3. Momentum</h3>
            <p class="first-parag">Momentum is an extension of Gradient Descent that makes learning smoother and faster by accumulating information from past updates. Instead of reacting only to the current gradient, the algorithm builds a sense of direction over time, helping the model move consistently toward lower loss.</p>
            <img src="assets/img/momentum.jpg" alt="momentum" class="small-section-image"> 
            <p>The core idea is simple: each update is influenced not only by the present gradient but also by a fraction of the previous update. This accumulated ‚Äúvelocity‚Äù allows the optimization to flow through shallow regions and resist oscillations in directions where the gradient frequently changes sign. As a result, Momentum tends to accelerate progress along stable paths and dampen noisy, zigzag movements.</p>
            <p>Mathematically, the algorithm maintains a velocity term that keeps track of past gradients. When the model encounters a long, gentle slope in the loss landscape, this velocity grows and pushes the network forward more effectively than plain Gradient Descent. Conversely, in directions where the gradient fluctuates or forms steep walls, the accumulated momentum slows down the updates, providing stability and preventing erratic jumps.</p>
            <p>Momentum is particularly helpful when the loss surface has narrow valleys‚Äîcommon in deep networks‚Äîwhere one direction is steep and the other is shallow. Plain Gradient Descent wastes time oscillating across the steep direction, advancing very slowly along the shallow one. By remembering the consistent component of the gradient, Momentum helps the model move forward steadily instead of vibrating sideways.</p>
            <p>Overall, Momentum improves both the speed and the reliability of training. It gives the optimization process a sense of inertia, allowing the network to advance through difficult regions of the loss landscape with more confidence and less noise.</p>
        </section>

        <section id="adam">
            <h3>1.4. Adam</h3>
            <p class="first-parag">Adam (Adaptive Moment Estimation) is an adaptive optimization algorithm that adjusts both the direction and the size of each weight update. It combines the stability of Momentum with the flexibility of per-parameter learning rates, allowing neural networks to train efficiently even in complex, noisy loss landscapes.</p>
            <p>Adam keeps two moving averages during training: one for the gradients themselves (capturing direction, like Momentum) and another for the squared gradients (capturing how large or variable they are). These two estimates help the optimizer decide not only where to move, but how big each step should be. Parameters with consistently large gradients receive smaller updates, while parameters with small or unreliable gradients are allowed to move more freely.</p>
            <p>Because Adam adapts the learning rate for each individual weight, it handles situations where different parameters evolve at very different scales. This is especially useful in deep networks, where some layers may require tiny adjustments while others benefit from more aggressive steps. The combination of momentum-like smoothing and adaptive scaling helps the optimizer remain stable even in highly irregular regions of the loss surface.</p>
            <p>Another advantage is that Adam works well with noisy gradient estimates, such as those produced by minibatches. The algorithm filters this noise through its moving averages, producing updates that are more consistent and less sensitive to momentary fluctuations. This makes training faster to converge and often more robust.</p>
            <p>In practice, Adam has become one of the most widely used optimizers in deep learning because it requires little tuning and performs reliably across many architectures and tasks. Although not always the best choice for final performance, its convenience and stability make it a common default in modern workflows.</p>
        </section>

        <section id="rate">
            <h3>1.5. Learning Rate and Learning Rate Scheduling</h3>
            <p class="first-parag">Learning Rate Scheduling is the strategy of adjusting the learning rate during training instead of keeping it fixed. The goal is simple: start with steps large enough to make fast progress, and gradually reduce them to allow the model to refine its solution without overshooting or oscillating.</p>
            <p>During the early stages of training, a relatively high learning rate helps the optimizer explore the loss landscape quickly, covering large distances and escaping shallow traps. However, as the model approaches regions of lower loss, these large steps become counterproductive. They can cause instability, prevent convergence, or bounce the parameters around a good solution without ever settling. Scheduling solves this by steadily decreasing the learning rate as training progresses, enabling both speed and precision.</p>
            <p>There are many scheduling strategies. Some reduce the learning rate at fixed intervals, while others respond directly to training performance‚Äîfor instance, lowering the rate when the loss plateaus. More advanced approaches modify the rate in smooth curves, such as exponential decay or cosine annealing, allowing the training dynamics to evolve more gradually. Despite their differences, all schedules share the same intention: match the step size to the needs of each phase of learning.</p>
            <p>A well-chosen schedule can dramatically improve training stability and final accuracy. It helps avoid the pitfalls of a single fixed learning rate, which is often too large for late training and too small for early exploration. By controlling how aggressively the optimizer moves through the loss landscape, learning rate scheduling becomes a key mechanism for efficient and reliable convergence.</p>
        </section>

    
    
    </section>
    </main>
 

    <footer>
        <p>&copy; 2025 Ana Princival</p>
    </footer>
</body>
</html>
